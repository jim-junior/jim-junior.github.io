# Deep Neural Networks

Contain multiple hiden layers

## Hyperparameters

K layers =  depth of network

Dk hidden units per layer = width of networks

> We dont need the same number of units in each layer

They are called hyperparamenters because we know then before training.

In training, we sometimes change the hyperparamenters to find the best hypaparamaters and this is called hyper parameter search or optimisation.
